{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deepak-Mewada/NeuralDecoder/blob/main/BrainDecoder/Basics/plot_bcic_iv_4_ecog_trial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkdKHl1xakXw"
      },
      "source": [
        "\n",
        "# Fingers flexion decoding on BCIC IV 4 ECoG Dataset\n",
        "\n",
        "This tutorial shows you how to train and test deep learning models with\n",
        "Braindecode on ECoG BCI IV competition dataset 4. For this dataset we will\n",
        "predict 5 regression targets corresponding to flexion of each finger.\n",
        "The targets were recorded as a time series (each 25 Hz), so this tutorial is\n",
        "an example of time series target prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExIa6BnTakYA"
      },
      "source": [
        "## Loading and preparing the dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC6tDMjsakYB"
      },
      "source": [
        "### Loading\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVQjrGsYakYC"
      },
      "source": [
        "First, we load the data. In this tutorial, we use the functionality of braindecode\n",
        "to load [BCI IV competition dataset 4](http://www.bbci.de/competition/iv/#dataset4)_.\n",
        "The dataset is available as a part of ECoG library:\n",
        "https://searchworks.stanford.edu/view/zk881ps0522\n",
        "\n",
        "The dataset contains ECoG signal and time series of 5 targets corresponding\n",
        "to each finger flexion. This is different than standard decoding setup for EEG with\n",
        "multiple trials and usually one target per trial. Here, fingers flexions change in time\n",
        "and are recorded with sampling frequency equals to 25 Hz.\n",
        "\n",
        "If this dataset is used please cite [1].\n",
        "\n",
        "[1] Miller, Kai J. \"A library of human electrocorticographic data and analyses.\n",
        "\"Nature human behaviour 3, no. 11 (2019): 1225-1235. https://doi.org/10.1038/s41562-019-0678-3\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxsP-rDXakYD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from braindecode.datasets import BCICompetitionIVDataset4\n",
        "\n",
        "subject_id = 1\n",
        "dataset = BCICompetitionIVDataset4(subject_ids=[subject_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALnuz1NQakYE"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdLifAStakYG"
      },
      "source": [
        "Now we apply preprocessing like bandpass filtering to our dataset. You\n",
        "can either apply functions provided by\n",
        "[mne.Raw](https://mne.tools/stable/generated/mne.io.Raw.html)_ or\n",
        "[mne.Epochs](https://mne.tools/0.11/generated/mne.Epochs.html#mne.Epochs)_\n",
        "or apply your own functions, either to the MNE object or the underlying\n",
        "numpy array.\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Preprocessing steps are taken from a standard EEG processing pipeline.\n",
        "   The only change is the cutoff frequency of the filter. For a proper ECoG\n",
        "   decoding other preprocessing steps may be needed.</p></div>\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>These prepocessings are now directly applied to the loaded\n",
        "   data, and not on-the-fly applied as transformations in\n",
        "   PyTorch-libraries like\n",
        "   [torchvision](https://pytorch.org/docs/stable/torchvision/index.html)_.</p></div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O74XPK3RakYH"
      },
      "outputs": [],
      "source": [
        "from braindecode.preprocessing import (Preprocessor,\n",
        "                                       exponential_moving_standardize,\n",
        "                                       preprocess)\n",
        "\n",
        "low_cut_hz = 1.\n",
        "high_cut_hz = 200.\n",
        "\n",
        "factor_new = 1e-3\n",
        "init_block_size = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_IJ1zAVakYL"
      },
      "source": [
        "We select only first 30 seconds from each dataset to limit time and memory\n",
        "to run this example. To obtain results on the whole datasets you should remove this line.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfOFFlpOakYM"
      },
      "outputs": [],
      "source": [
        "preprocess(dataset, [Preprocessor('crop', tmin=0, tmax=30)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwhg-tuaakYO"
      },
      "source": [
        "In time series targets setup, targets variables are stored in mne.Raw object as channels\n",
        "of type `misc`. Thus those channels have to be selected for further processing. However,\n",
        "many mne functions ignore `misc` channels and perform operations only on data channels\n",
        "(see https://mne.tools/stable/glossary.html#term-data-channels).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6I9VDz0akYO"
      },
      "outputs": [],
      "source": [
        "preprocessors = [\n",
        "    Preprocessor('pick_types', ecog=True, misc=True),\n",
        "    Preprocessor(lambda x: x / 1e6, picks='ecog'),  # Convert from V to uV\n",
        "    Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz),  # Bandpass filter\n",
        "    Preprocessor(exponential_moving_standardize,  # Exponential moving standardization\n",
        "                 factor_new=factor_new, init_block_size=init_block_size, picks='ecog')\n",
        "]\n",
        "\n",
        "# Transform the data\n",
        "preprocess(dataset, preprocessors)\n",
        "\n",
        "# Extract sampling frequency, check that they are same in all datasets\n",
        "sfreq = dataset.datasets[0].raw.info['sfreq']\n",
        "assert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])\n",
        "# Extract target sampling frequency\n",
        "target_sfreq = dataset.datasets[0].raw.info['temp']['target_sfreq']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GROpXmTuakYP"
      },
      "source": [
        "### Cut Compute Windows\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkeJkYmRakYQ"
      },
      "source": [
        "Now we cut out compute windows, the inputs for the deep networks during\n",
        "training. In the case of trialwise decoding of time series targets, we just have to\n",
        "decide about length windows that will be selected from the signal preceding each target.\n",
        "We use different windowing function than in standard trialwise decoding as our targets\n",
        "are stored as target channels in mne.Raw.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUtE4AnQakYR"
      },
      "outputs": [],
      "source": [
        "from braindecode.preprocessing import create_windows_from_target_channels\n",
        "\n",
        "windows_dataset = create_windows_from_target_channels(\n",
        "    dataset,\n",
        "    window_size_samples=1000,\n",
        "    preload=False,\n",
        "    last_target_only=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7nNEW2cakYR"
      },
      "source": [
        "We select only the thumb's finger flexion to create one model per finger.\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Methods to predict all 5 fingers flexion with the same model may be cnosidered as well.\n",
        "   We encourage you to find your own way to use braindecode models to predict finers fexions.</p></div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfIgyVzZakYS"
      },
      "outputs": [],
      "source": [
        "windows_dataset.target_transform = lambda x: x[0: 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avVYBakvakYT"
      },
      "source": [
        "### Split dataset into train, valid, and test\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O03WwWWfakYT"
      },
      "source": [
        "We can easily split the dataset using additional info stored in the\n",
        "description attribute, in this case ``session`` column. We select `train` dataset\n",
        "for training and validation and `test` for final evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odVIAR5eakYU"
      },
      "outputs": [],
      "source": [
        "subsets = windows_dataset.split('session')\n",
        "train_set = subsets['train']\n",
        "test_set = subsets['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QPuPtEUakYU"
      },
      "source": [
        "We can split train dataset into training and validation datasets using\n",
        "``sklearn.model_selection.train_test_split`` and ``torch.utils.data.Subset``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfKNkmIOakYV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "idx_train, idx_valid = train_test_split(np.arange(len(train_set)),\n",
        "                                        random_state=100,\n",
        "                                        test_size=0.2,\n",
        "                                        shuffle=False)\n",
        "\n",
        "valid_set = torch.utils.data.Subset(train_set, idx_valid)\n",
        "train_set = torch.utils.data.Subset(train_set, idx_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypzIGS14akYW"
      },
      "source": [
        "## Create model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkbS0CM9akYW"
      },
      "source": [
        "Now we create the deep learning model! Braindecode comes with some\n",
        "predefined convolutional neural network architectures for raw\n",
        "time-domain EEG. Here, we use the shallow ConvNet model from [Deep\n",
        "learning with convolutional neural networks for EEG decoding and\n",
        "visualization](https://arxiv.org/abs/1703.05051)_. These models are\n",
        "pure [PyTorch](https://pytorch.org)_ deep learning models, therefore\n",
        "to use your own model, it just has to be a normal PyTorch\n",
        "[nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)_.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXanXKGLakYX"
      },
      "outputs": [],
      "source": [
        "from braindecode.models import ShallowFBCSPNet\n",
        "from braindecode.util import set_random_seeds\n",
        "\n",
        "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
        "device = 'cuda' if cuda else 'cpu'\n",
        "if cuda:\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "# Set random seed to be able to roughly reproduce results\n",
        "# Note that with cudnn benchmark set to True, GPU indeterminism\n",
        "# may still make results substantially different between runs.\n",
        "# To obtain more consistent results at the cost of increased computation time,\n",
        "# you can set `cudnn_benchmark=False` in `set_random_seeds`\n",
        "# or remove `torch.backends.cudnn.benchmark = True`\n",
        "seed = 20200220\n",
        "set_random_seeds(seed=seed, cuda=cuda)\n",
        "\n",
        "n_out_chans = train_set[0][1].shape[0]\n",
        "# Extract number of chans and time steps from dataset\n",
        "n_chans = train_set[0][0].shape[0]\n",
        "input_window_samples = 1000  # 1 second long windows\n",
        "\n",
        "model = ShallowFBCSPNet(\n",
        "    n_chans,\n",
        "    n_out_chans,\n",
        "    input_window_samples=input_window_samples,\n",
        "    final_conv_length='auto',\n",
        "    add_log_softmax=False,\n",
        ")\n",
        "\n",
        "# Send model to GPU\n",
        "if cuda:\n",
        "    model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLYWkyWrakYY"
      },
      "source": [
        "## Training\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVdHmNYpakYZ"
      },
      "source": [
        "Now we train the network! EEGRegressor is a Braindecode object\n",
        "responsible for managing the training of neural networks. It inherits\n",
        "from skorch.NeuralNetRegressor, so the training logic is the same as in\n",
        "[Skorch](https://skorch.readthedocs.io/en/stable/)_.\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>In this tutorial, we use some default parameters that we\n",
        "   have found to work well for EEG motor decoding, however we strongly\n",
        "   encourage you to perform your own hyperparameter and preprocessing optimization using\n",
        "   cross validation on your training data.</p></div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmzyK_2WakYa"
      },
      "outputs": [],
      "source": [
        "from skorch.callbacks import EpochScoring, LRScheduler\n",
        "from skorch.helper import predefined_split\n",
        "from mne import set_log_level\n",
        "\n",
        "from braindecode import EEGRegressor\n",
        "\n",
        "# These values we found good for shallow network for EEG MI decoding:\n",
        "lr = 0.0625 * 0.01\n",
        "weight_decay = 0\n",
        "batch_size = 64\n",
        "n_epochs = 2\n",
        "\n",
        "\n",
        "# Function to compute Pearson correlation coefficient\n",
        "def pearson_r_score(net, dataset, y):\n",
        "    preds = net.predict(dataset)\n",
        "    corr_coeffs = []\n",
        "    for i in range(y.shape[1]):\n",
        "        corr_coeffs.append(np.corrcoef(y[:, i], preds[:, i])[0, 1])\n",
        "    return np.mean(corr_coeffs)\n",
        "\n",
        "\n",
        "regressor = EEGRegressor(\n",
        "    model,\n",
        "    criterion=torch.nn.MSELoss,\n",
        "    optimizer=torch.optim.AdamW,\n",
        "    train_split=predefined_split(valid_set),  # using valid_set for validation,\n",
        "    optimizer__lr=lr,\n",
        "    optimizer__weight_decay=weight_decay,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=[\n",
        "        'r2',\n",
        "        ('valid_pearson_r', EpochScoring(pearson_r_score, lower_is_better=False, on_train=False,\n",
        "                                         name='valid_pearson_r')),\n",
        "        ('train_pearson_r', EpochScoring(pearson_r_score, lower_is_better=False, on_train=True,\n",
        "                                         name='train_pearson_r')),\n",
        "        (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
        "    ],\n",
        "    device=device,\n",
        ")\n",
        "set_log_level(verbose='WARNING')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kChQAjUEakYc"
      },
      "source": [
        "Model training for a specified number of epochs. ``y`` is None as it is already supplied\n",
        "in the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHXGX32aakYd"
      },
      "outputs": [],
      "source": [
        "regressor.fit(train_set, y=None, epochs=n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_KK89h3akYf"
      },
      "source": [
        "Obtaining predictions and targets for the test, train, and validation dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqcp25nJakYg"
      },
      "outputs": [],
      "source": [
        "preds_test = regressor.predict(test_set)\n",
        "y_test = np.stack([data[1] for data in test_set])\n",
        "preds_train = regressor.predict(train_set)\n",
        "y_train = np.stack([data[1] for data in train_set])\n",
        "preds_valid = regressor.predict(valid_set)\n",
        "y_valid = np.stack([data[1] for data in valid_set])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNMYipViakYh"
      },
      "source": [
        "## Plot Results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5a1b-JPakYh"
      },
      "source": [
        "We plot target and predicted finger flexion on training, validation, and test sets.\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>The model is trained and validated on limited dataset (to decrease the time needed to run\n",
        "   this example) which does not contain diverse dataset in terms of fingers flexions and may\n",
        "   cause overfitting. To obtain better results use whole dataset as well as improve the decoding\n",
        "   pipeline which may be not optimal for ECoG.</p></div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6AXV9DiakYi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from matplotlib.lines import Line2D\n",
        "\n",
        "fig, axes = plt.subplots(3, 1, figsize=(8, 9))\n",
        "\n",
        "axes[0].set_title('Training dataset')\n",
        "axes[0].plot(np.arange(0, y_train.shape[0]) / target_sfreq, y_train[:, 0], label='Target')\n",
        "axes[0].plot(np.arange(0, preds_train.shape[0]) / target_sfreq, preds_train[:, 0],\n",
        "             label='Predicted')\n",
        "axes[0].set_ylabel('Finger flexion')\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].set_title('Validation dataset')\n",
        "axes[1].plot(np.arange(0, y_valid.shape[0]) / target_sfreq, y_valid[:, 0], label='Target')\n",
        "axes[1].plot(np.arange(0, preds_valid.shape[0]) / target_sfreq, preds_valid[:, 0],\n",
        "             label='Predicted')\n",
        "axes[1].set_ylabel('Finger flexion')\n",
        "axes[1].legend()\n",
        "\n",
        "axes[2].set_title('Test dataset')\n",
        "axes[2].plot(np.arange(0, y_test.shape[0]) / target_sfreq, y_test[:, 0], label='Target')\n",
        "axes[2].plot(np.arange(0, preds_test.shape[0]) / target_sfreq, preds_test[:, 0], label='Predicted')\n",
        "axes[2].set_xlabel('Time [s]')\n",
        "axes[2].set_ylabel('Finger flexion')\n",
        "axes[2].legend()\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrr3zoZQakYj"
      },
      "source": [
        "We can compute correlation coefficients for each finger\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa3QXum8akYk"
      },
      "outputs": [],
      "source": [
        "corr_coeffs = []\n",
        "for dim in range(y_test.shape[1]):\n",
        "    corr_coeffs.append(\n",
        "        np.corrcoef(preds_test[:, dim], y_test[:, dim])[0, 1]\n",
        "    )\n",
        "print('Correlation coefficient for each dimension: ', np.round(corr_coeffs, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Se06A9RakYk"
      },
      "source": [
        "Now we use the history stored by Skorch throughout training to plot\n",
        "accuracy and loss curves.\n",
        "Extract loss and accuracy values for plotting from history object\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9bAK_lvakY3"
      },
      "outputs": [],
      "source": [
        "results_columns = ['train_loss', 'valid_loss', 'train_pearson_r', 'valid_pearson_r']\n",
        "df = pd.DataFrame(regressor.history[:, results_columns], columns=results_columns,\n",
        "                  index=regressor.history[:, 'epoch'])\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(8, 4))\n",
        "df.loc[:, ['train_loss', 'valid_loss']].plot(\n",
        "    ax=ax1, style=['-', ':'], marker='o', color='tab:blue', legend=False, fontsize=14)\n",
        "\n",
        "ax1.tick_params(axis='y', labelcolor='tab:blue', labelsize=14)\n",
        "ax1.set_ylabel(\"Loss\", color='tab:blue', fontsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "df.loc[:, ['train_pearson_r', 'valid_pearson_r']].plot(\n",
        "    ax=ax2, style=['-', ':'], marker='o', color='tab:red', legend=False)\n",
        "ax2.tick_params(axis='y', labelcolor='tab:red', labelsize=14)\n",
        "ax2.set_ylabel(\"Pearson correlation coefficient\", color='tab:red', fontsize=14)\n",
        "ax1.set_xlabel(\"Epoch\", fontsize=14)\n",
        "\n",
        "# where some data has already been plotted to ax\n",
        "handles = []\n",
        "handles.append(Line2D([0], [0], color='black', linewidth=1, linestyle='-',\n",
        "                      label='Train'))\n",
        "handles.append(Line2D([0], [0], color='black', linewidth=1, linestyle=':',\n",
        "                      label='Valid'))\n",
        "plt.legend(handles, [h.get_label() for h in handles], fontsize=14, loc='center right')\n",
        "plt.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}